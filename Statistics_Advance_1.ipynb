{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a random variable in probability theory?\n",
        "   - In probability theory, a random variable is a variable that represents the outcome of a random experiment.\n",
        "   - It assigns a numerical value to each possible outcome.\n",
        "\n",
        "2. What are the types of random variables?\n",
        "   - Types of random variables are as follows:\n",
        "      - 1. Discrete Random Variable:\n",
        "      - Takes countable values (like integers).\n",
        "      - Example: Number of heads in 3 coin tosses, Values: 0, 1, 2, 3\n",
        "      - 2. Continuous Random Variable:\n",
        "      - Takes uncountable values within a range (real numbers).\n",
        "      - Example: Time it takes for a bus to arrive, Values: any real number like 2.3 minutes, 5.75 minutes\n",
        "\n",
        "3. What is the difference between discrete and continuous distributions?\n",
        "   - Discrete Distribution:\n",
        "     - Takes countable values (like whole numbers).\n",
        "     - Probabilities are assigned to individual outcomes.\n",
        "     - Represented using Probability Mass Function (PMF).\n",
        "     - Examples - Dice roll, number of emails.\n",
        "   - Continuous Distribution:\n",
        "     - Takes infinite uncountable values in an interval.\n",
        "     - Probabilities are assigned over ranges.\n",
        "     - Represented using Probability Density Function (PDF).\n",
        "     - Examples - Height, time, temperature.\n",
        "\n",
        "4. What are probability distribution functions (PDF)?\n",
        "   - A Probability Distribution Function (PDF) describes how the values of a continuous random variable are distributed. It shows the likelihood of a variable taking on a specific range of values.\n",
        "   - PDF is used for Continuous Random Variables.\n",
        "   - It tells how dense the probability is near a certain value — not the actual probability of a value.\n",
        "   - The probability of the variable being exactly equal to any specific value is 0.\n",
        "   - To find the probability over a range, you calculate the area under the curve of the PDF using integration.\n",
        "\n",
        "5. How do cumulative distribution functions (CDF) differ from probability distribution functions (PDF)?\n",
        "   - PDF (Probability Density Function):\n",
        "      - Describes the density of probabilities at each point for continuous variables.\n",
        "      - Tells how likely a value is to occur near a point.\n",
        "      - f(x)=Probability density at x\n",
        "      - PDF is a curve (like a bell curve in normal distribution).\n",
        "      - It can be any non-negative value (not limited to 1).\n",
        "   - CDF (Cumulative Distribution Function):\n",
        "      - Describes the cumulative probability up to a value.\n",
        "      - Tells the probability that a variable is less than or equal to a certain value.\n",
        "      - It's the area under the PDF curve from −∞ to x.\n",
        "      - CDF is a non-decreasing curve that goes from 0 to 1 as x increases.\n",
        "      - Value is always between 0 and 1.\n",
        "\n",
        "6. What is a discrete uniform distribution?\n",
        "   - A discrete uniform distribution is a type of probability distribution in which all outcomes are equally likely over a finite set of values.\n",
        "   - Examples: Rolling a fair dice, Picking a card at random from a shuffled subset\n",
        "\n",
        "7. What are the key properties of a Bernoulli distribution?\n",
        "   - A Bernoulli distribution is the simplest probability distribution — it models a single experiment (trial) that has only two possible outcomes: success (1) or failure (0).\n",
        "   - It models experiments with exactly two outcomes — typically labeled as 1 (success) and 0 (failure).\n",
        "   - The probability of success is denoted by p, while the probability of failure is 1-p.\n",
        "   - The mean or expected value of a Bernoulli random variable is p.\n",
        "   - The variance of the distribution is p(1-p), which captures the spread of outcomes.\n",
        "   - It is defined for values X∈{0,1}, meaning the variable can only take 0 or 1.\n",
        "   - Use Cases: Yes/No surveys, Pass/Fail tests, Click/No-click in digital ads\n",
        "\n",
        "8. What is the binomial distribution, and how is it used in probability?\n",
        "   - The binomial distribution is a discrete probability distribution that models the number of successes in a fixed number of independent Bernoulli trials, where each trial has the same probability of success.\n",
        "   - It answers the question: \"What is the probability of getting exactly k successes in n trials?\"\n",
        "   - Each trial has only two possible outcomes: success (with probability p) or failure (with probability 1-p).\n",
        "   - The trials are independent, meaning the outcome of one trial does not affect others.\n",
        "   - Suppose a coin is flipped 10 times. The binomial distribution can calculate the probability of getting exactly 6 heads (assuming the coin is fair, so p=0.5).\n",
        "   - Applications:\n",
        "       - Quality control (e.g., number of defective items in a batch)\n",
        "       - Marketing (e.g., number of people responding to a campaign)\n",
        "       - Medicine (e.g., number of patients responding to treatment)\n",
        "\n",
        "9. What is the Poisson distribution and where is it applied?\n",
        "   - The Poisson distribution is a discrete probability distribution that models the number of times an event occurs in a fixed interval of time or space, given that the events occur independently and at a constant average rate.\n",
        "   - It is applied in following situations:\n",
        "   - Number of emails received per hour\n",
        "   - Number of phone calls at a call center per minute\n",
        "   - Number of accidents at a traffic intersection per month\n",
        "   - Number of typos per page in a book\n",
        "\n",
        "10. What is a continuous uniform distribution?\n",
        "    - A continuous uniform distribution is a probability distribution where all values within a given interval are equally likely to occur. It is defined over a continuous range [a,b].\n",
        "    - Probability density function (PDF) is:\n",
        "                    f(x)= 1/(b-a) for a ≤ x ≤ b\n",
        "    - Equal probability for all values in [a,b]\n",
        "    - Mean = a+b/2\n",
        "    - Variance = (b-a)2/12\n",
        "    - Used in simulations and modeling random events with no preference within the range.\n",
        "\n",
        "11. What are the characteristics of a normal distribution?\n",
        "    - The normal distribution is also known as the Gaussian distribution.\n",
        "    - 1. Bell-shaped and symmetric: The curve is perfectly symmetrical around the mean.\n",
        "    - 2. Mean = Median = Mode: All three central measures are equal and located at the center.\n",
        "    - 3. Defined by two parameters: Mean (μ): Determines the center of the distribution. Standard Deviation (σ): Determines the spread or width of the curve.\n",
        "    - 4. Empirical Rule (68-95-99.7 rule): ~68% of data lies within 1σ of the mean. ~95% within 2σ. ~99.7% within 3σ.\n",
        "    - 5. Asymptotic: The tails approach the horizontal axis but never touch it.- 6. Total area under the curve = 1: This represents the total probability.- 7. Unimodal: Has a single peak.\n",
        "\n",
        "12. What is the standard normal distribution, and why is it important?\n",
        "    - The standard normal distribution is a special case of the normal distribution that has: Mean (μ) = 0, Standard deviation (σ) = 1\n",
        "    - Simplifies calculations: Many statistical formulas and tables (like Z-tables) are based on the standard normal distribution.\n",
        "    - Used for Z-scores: Any normal distribution can be converted into a standard normal distribution using:\n",
        "                      Z= X-μ/σ\n",
        "          This helps compare different data points across distributions.\n",
        "    - Forms the basis for hypothesis testing and confidence intervals.\n",
        "    - Universal reference: It's a standardized way to interpret where a value lies in relation to the mean.\n",
        "\n",
        "13. What is the Central Limit Theorem (CLT), and why is it critical in statistics?\n",
        "    - When independent random variables are added, their normalized sum tends toward a normal distribution — even if the original variables themselves are not normally distributed — as the sample size becomes large.\n",
        "    - The sample mean of a large number of independent, identically distributed variables will be approximately normally distributed.\n",
        "    - This applies regardless of the original distribution of the population (as long as the variance is finite).\n",
        "    - The approximation improves as sample size (n) increases (usually n ≥ 30 is considered sufficient).\n",
        "    - It is critical because of the following reasons:\n",
        "    - Enables use of normal distribution for inference (e.g., confidence intervals, hypothesis testing).\n",
        "    - Justifies the use of Z-scores and t-tests even when the population distribution is unknown.\n",
        "    - Supports many real-world statistical applications — like quality control, surveys, and financial modeling.\n",
        "\n",
        "14. How does the Central Limit Theorem relate to the normal distribution?\n",
        "    - The Central Limit Theorem (CLT) explains how the normal distribution emerges naturally when dealing with the means of repeated samples.\n",
        "    - It allows statisticians to use the normal distribution to:\n",
        "          - Construct confidence intervals.\n",
        "          - Perform hypothesis testing.\n",
        "          - Use Z-scores and t-tests.\n",
        "    - This is especially useful when the population distribution is unknown or non-normal.\n",
        "    - According to CLT, no matter the shape of the original population distribution, the distribution of the sample means will tend to be normal as the sample size increases.\n",
        "    - This means that even if the population is skewed, uniform, or irregular, the sampling distribution of the mean will become approximately normal as the number of observations (n) grows.\n",
        "\n",
        "15. What is the application of Z statistics in hypothesis testing?\n",
        "    - Z-statistics (or Z-score) is used in hypothesis testing to determine whether a sample mean significantly differs from a population mean, assuming the population standard deviation is known.\n",
        "    - It standardizes the difference between the sample statistic and the population parameter using standard deviation.\n",
        "    - Z-tests are appropriate when sample size is large (n ≥ 30) or population standard deviation is known.\n",
        "    - The Z-value is compared against a critical value from the standard normal distribution to accept or reject the null hypothesis.\n",
        "    - Z-statistics help quantify how many standard deviations a sample mean is from the population mean, enabling clear decisions in hypothesis testing when population parameters are known.\n",
        "\n",
        "16. How do you calculate a Z-score, and what does it represent?\n",
        "    - A Z-score tells how many standard deviations a data point is from the mean of the distribution.\n",
        "    - Z = X-μ/σ\n",
        "             - X = the data value\n",
        "             - μ = mean of the population\n",
        "             - σ = standard deviation of the population\n",
        "    - It standardizes values from different distributions to a common scale.\n",
        "    - Helps identify outliers.\n",
        "    - Used in hypothesis testing, normal distribution analysis, and comparison of different data points.\n",
        "\n",
        "17. What are point estimates and interval estimates in statistics?\n",
        "    - Point Estimate: A point estimate is a single value used to estimate a population parameter. It gives a specific number.\n",
        "    - Example: We survey 100 people and find the average height is 165 cm, Point estimate of the population mean height = 165 cm.\n",
        "    - Interval Estimate: An interval estimate provides a range of values within which the population parameter is likely to lie, usually with a confidence level (e.g., 95%). Gives upper and lower bounds. Includes margin of error.More informative and reliable than a point estimate.\n",
        "    - Example: A 95% confidence interval for population mean height is (162 cm, 168 cm). It's 95% confident the true mean is between 162 and 168 cm.\n",
        "\n",
        "18. What is the significance of confidence intervals in statistical analysis?\n",
        "    - Confidence intervals (CIs) play a critical role in statistical analysis because they provide a range of values that is likely to contain the true population parameter (like a mean or proportion), along with a specified level of confidence (usually 90%, 95%, or 99%).\n",
        "    - Example: We estimate the average weight of a population as 70 kg with a 95% CI of (68, 72). This means we are 95% confident that the true average weight lies between 68 kg and 72 kg.\n",
        "    - Confidence intervals add context and reliability to the analysis, making the conclusions more meaningful and defensible.\n",
        "    - 1. Shows Precision of Estimates:\n",
        "           - A narrower CI means the estimate is more precise.\n",
        "           - A wider CI shows more variability or uncertainty.\n",
        "    - 2. Accounts for Sampling Variability:\n",
        "           - Instead of relying on just one number (point estimate), CI gives a range that reflects possible error due to sampling.\n",
        "    - 3. Helps in Decision-Making:\n",
        "          - If a CI does not include a value like 0 or a specific benchmark, it may support or reject a hypothesis (e.g., treatment effectiveness).\n",
        "    - 4. Indicates Reliability:\n",
        "          - A 95% confidence interval means that if the same population were sampled multiple times, about 95% of the calculated intervals would contain the true parameter.\n",
        "    - 5. Used in Hypothesis Testing:\n",
        "          - CIs complement p-values and help assess statistical significance and practical relevance.\n",
        "\n",
        "19. What is the relationship between a Z-score and a confidence interval?\n",
        "    - The Z-score and confidence interval are directly related in statistics because Z-scores define the boundaries of confidence intervals when the population standard deviation is known or the sample size is large.\n",
        "    - The Z-score determines how confident we want to be.\n",
        "    - It is used to calculate the confidence interval bounds.\n",
        "    - Without the Z-score, we can't establish the CI for a normal distribution.- Z-scores are essential building blocks for confidence intervals.\n",
        "    - Z-scores determine the confidence level: A Z-score tells us how many standard deviations a value is from the mean. In confidence intervals, Z-scores are used to set the margin of error.\n",
        "    - Standard Z-scores for Common Confidence Levels:\n",
        "              Confidence_Level\t    Z-score (two-tailed)\n",
        "                     90%\t             ±1.645\n",
        "                     95%\t             ±1.96\n",
        "                     99%\t             ±2.576\n",
        "    - Confidence Interval Formula (using Z):\n",
        "                  CI = x ± Z * 𝜎/root of n\n",
        "                  Where:\n",
        "                       x = sample mean\n",
        "                       Z = Z-score based on desired confidence level\n",
        "                       σ = population standard deviation\n",
        "                       n = sample size\n",
        "   - Interpretation:\n",
        "            - The higher the Z-score, the wider the confidence interval.\n",
        "            - This reflects more confidence, but also greater uncertainty in precision.\n",
        "\n",
        "20. How are Z-scores used to compare different distributions?\n",
        "    - Z-scores allow us to standardize different data points from different distributions, making them comparable on the same scale, regardless of the original units or distribution shapes.\n",
        "    - A Z-score is the number of standard deviations a data point is from the mean of its distribution.\n",
        "    - Z-scores transform values from different distributions to a standard normal distribution with a mean of 0 and a standard deviation of 1.\n",
        "    - Z-scores normalize values from different datasets.\n",
        "    - They enable fair comparisons by eliminating the effect of different units and scales.\n",
        "    - Widely used in testing, grading, quality control, and anomaly detection.\n",
        "\n",
        "21. What are the assumptions for applying the Central Limit Theorem?\n",
        "    - 1. Random Sampling: The data should be collected using random sampling techniques to avoid bias.\n",
        "    - 2. Independence: Observations in the sample must be independent of each other. This means the outcome of one observation should not affect another.- 3. Sample Size: The sample size should be \"sufficiently large\". A common rule of thumb: If the population distribution is not normal, a sample size of n ≥ 30 is often enough. If the population is normal, even a small sample size works.\n",
        "    - 4. Finite Variance: The population from which the samples are drawn must have a finite variance (not infinite or undefined).\n",
        "\n",
        "22. What is the concept of expected value in a probability distribution?\n",
        "    - The expected value (or mean) of a probability distribution represents the long-run average outcome of a random variable if an experiment is repeated many times.\n",
        "    - For a discrete random variable, the expected value is calculated as:\n",
        "                        E(X)=∑xi * P(xi)\n",
        "              Where: xi = value of the outcome, P(xi) = probability of that outcome\n",
        "\n",
        "23. How does a probability distribution relate to the expected outcome of a random variable?\n",
        "    - A probability distribution provides a complete picture of all possible outcomes of a random variable and how likely each outcome is. The expected outcome (or expected value) is a summary measure of that distribution—it tells us the average value we would expect if we repeated the random experiment many times.\n",
        "    - A random variable maps outcomes of a random process to numerical values.\n",
        "    - A probability distribution assigns probabilities to each of those possible values.\n",
        "    - The expected value is calculated by combining the values of the random variable with their corresponding probabilities.\n",
        "    - For discrete variables: E(X)=∑xi * P(xi)\n",
        "    - For continuous variables: E(X)=∫x * f(x)dx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JpVb-cZegxfB"
      }
    }
  ]
}